<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>data · PyTorchVideo</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="# Overview"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="data · PyTorchVideo"/><meta property="og:type" content="website"/><meta property="og:url" content="https://pytorchvideo.org/"/><meta property="og:description" content="# Overview"/><meta property="og:image" content="https://pytorchvideo.org/img/logo.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://pytorchvideo.org/img/logo.svg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo.svg" alt="PyTorchVideo"/><h2 class="headerTitleWithLogo">PyTorchVideo</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/tutorial_overview" target="_self">Tutorials</a></li><li class=""><a href="/docs_redirect" target="_self">Docs</a></li><li class=""><a href="https://github.com/facebookresearch/pytorchvideo/" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">data</h1></header><article><div><span><h1><a class="anchor" aria-hidden="true" id="overview"></a><a href="#overview" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Overview</h1>
<p>PyTorchVideo datasets are subclasses of either torch.utils.data.Dataset or torch.utils.data.IterableDataset. As such, they can all be used with a torch.utils.data.DataLoader, which can load multiple samples in parallel using torch.multiprocessing workers. For example:</p>
<pre><code class="hljs css language-python">dataset = pytorchvideo.data.Kinetics(
        data_path=<span class="hljs-string">"path/to/kinetics_root/train.csv"</span>,
        clip_sampler=pytorchvideo.data.make_clip_sampler(<span class="hljs-string">"random"</span>, duration=<span class="hljs-number">2</span>),
)
data_loader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">8</span>)
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="how-do-pytorchvideo-dataset-work"></a><a href="#how-do-pytorchvideo-dataset-work" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How do PyTorchVideo dataset work?</h2>
<p>Although there isn't a strict interface governing how PyTorchVideo datasets work, they all share a common design as follows:</p>
<ol>
<li><p>Each dataset starts by taking a list of video paths and labels in some form. For example, Kinetics can take a file with each row containing a video path and label, or a directory containing a &lt;label&gt;/&lt;video_name&gt;.mp4 like file structure. Each respective dataset documents the exact structure it expected for the given data path.</p></li>
<li><p>At each iteration a video sampler is used to determine which video-label pair is going to be sampled from the list of videos from the previous point. For some datasets this is required to be a random sampler, others reuse the torch.utils.data.Sampler interface for more flexibility.</p></li>
<li><p>A clip sampler is then used to determine which frames to sample from the selected video. For example, your application may want to sample 2 second clips at random for the selected video at each iteration. Some datasets like Kinetics make the most of the pytorchvideo.data.clip_sampling interface to provide flexibility on how to define these clips. Other datasets simply require you to specify an enum for common clip sampling configurations.</p></li>
<li><p>Depending on if the underlying videos are stored as either encoded videos (e.g. mp4) or frame videos (i.e. a folder of images containing each decoded frame) - the video clip is then selectively read or decoded into the canonical video tensor with shape (C, T, H, W) and audio tensor with shape (S). We provide two options for decoding: PyAv or TorchVision, which can be chosen in the interface of the datasets that supported encoded videos.</p></li>
<li><p>The next step of a PyTorchVideo dataset is creating a clip dictionary containing the video modalities, label and metadata ready to be returned. An example clip dictionary might look like this:</p>
<pre><code class="hljs">  {
     <span class="hljs-string">'video'</span>: &lt;video_tensor&gt;,     # Shape: (C, T, H, W)
     <span class="hljs-string">'audio'</span>: &lt;audio_tensor&gt;,     # Shape: (S)
     <span class="hljs-string">'label'</span>: &lt;action_label&gt;,     # Integer defining <span class="hljs-keyword">class</span> <span class="hljs-symbol">annotation</span>
     '<span class="hljs-symbol">video_name</span>': &lt;<span class="hljs-symbol">video_path</span>&gt;,  # <span class="hljs-symbol">Video</span> <span class="hljs-symbol">file</span> <span class="hljs-symbol">path</span> <span class="hljs-symbol">stem</span>
     '<span class="hljs-symbol">video_index</span>': &lt;<span class="hljs-symbol">video_id</span>&gt;,   # <span class="hljs-symbol">index</span> <span class="hljs-symbol">of</span> <span class="hljs-symbol">video</span> <span class="hljs-symbol">used</span> <span class="hljs-symbol">by</span> <span class="hljs-symbol">sampler</span>
     '<span class="hljs-symbol">clip_index</span>': &lt;<span class="hljs-symbol">clip_id</span>&gt;      # <span class="hljs-symbol">index</span> <span class="hljs-symbol">of</span> <span class="hljs-symbol">clip</span> <span class="hljs-symbol">sampled</span> <span class="hljs-symbol">within</span> <span class="hljs-symbol">video</span>
  }
</code></pre>
<p>All datasets share the same canonical modality tensor shapes and dtypes, which aligns with tensor types of other domain specific libraries (e.g. TorchVision, TorchAudio).</p></li>
<li><p>The final step before returning a clip, involves feeding it into a transform callable that can be defined for of all PyTorchVideo datasets. This callable is used to allow custom data processing or augmentations to be applied before batch collation in the torch.utils.data.DataLoader. PyTorchVideo provides common <a href="https://pytorchvideo.org/docs/transforms.html">transforms</a> that are useful for this callable, but users can easily define their own too.</p></li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="available-datasets"></a><a href="#available-datasets" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Available datasets:</h2>
<ul>
<li><a href="http://pytorchvideo.org/docs/api/data/charades.html#pytorchvideo-data-charades">Charades</a></li>
<li><a href="http://pytorchvideo.org/docs/api/data/domsev.html#module-pytorchvideo.data.domsev">Domsev</a></li>
<li><a href="http://pytorchvideo.org/docs/api/data/encoded_video.html#pytorchvideo-data-encoded-video-dataset">EpicKitchen</a></li>
<li><a href="http://pytorchvideo.org/docs/api/data/encoded_video.html#pytorchvideo-data-encoded-video-dataset">HMDB51</a></li>
<li><a href="http://pytorchvideo.org/docs/api/data/encoded_video.html#pytorchvideo-data-encoded-video-dataset">Kinetics</a></li>
<li>SSV2 (TODO)</li>
<li><a href="http://pytorchvideo.org/docs/api/data/encoded_video.html#pytorchvideo-data-encoded-video-dataset">UCF101</a></li>
</ul>
</span></div></article></div><div class="docs-prevnext"></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#how-do-pytorchvideo-dataset-work">How do PyTorchVideo dataset work?</a></li><li><a href="#available-datasets">Available datasets:</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div class="footerSection"><div class="social"><a class="github-button" href="https://github.com/facebookresearch/pytorchvideo" data-count-href="https://github.com/facebookresearch/pytorchvideo/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star PytorchVideo on GitHub">pytorchvideo</a></div></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2021 Facebook, Inc<br/>Legal:<a href="https://opensource.facebook.com/legal/privacy/" target="_blank" rel="noreferrer noopener">Privacy</a><a href="https://opensource.facebook.com/legal/terms/" target="_blank" rel="noreferrer noopener">Terms</a></section></footer></div></body></html>