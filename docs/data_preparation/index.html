<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>data_preparation · PyTorchVideo</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="## Data Preparation"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="data_preparation · PyTorchVideo"/><meta property="og:type" content="website"/><meta property="og:url" content="https://pytorchvideo.org/"/><meta property="og:description" content="## Data Preparation"/><meta property="og:image" content="https://pytorchvideo.org/img/logo.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://pytorchvideo.org/img/logo.svg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo.svg" alt="PyTorchVideo"/><h2 class="headerTitleWithLogo">PyTorchVideo</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/tutorial_overview" target="_self">Tutorials</a></li><li class=""><a href="/docs/index.html" target="_self">Docs</a></li><li class=""><a href="https://github.com/facebookresearch/pytorchvideo/" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">data_preparation</h1></header><article><div><span><h2><a class="anchor" aria-hidden="true" id="data-preparation"></a><a href="#data-preparation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data Preparation</h2>
<h3><a class="anchor" aria-hidden="true" id="kinetics"></a><a href="#kinetics" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Kinetics</h3>
<p>For more information about Kinetics dataset, please refer the official <a href="https://deepmind.com/research/open-source/kinetics">website</a>. You can take the following steps to prepare the dataset:</p>
<ol>
<li><p>Download the videos via the official <a href="https://github.com/activitynet/ActivityNet/tree/master/Crawler/Kinetics">scripts</a>.</p></li>
<li><p>Preprocess the downloaded videos by resizing to the short edge size of 256.</p></li>
<li><p>Prepare the csv files for training, validation, and testing set as <code>train.csv</code>, <code>val.csv</code>, <code>test.csv</code>. The format of the csv file is:</p></li>
</ol>
<pre><code class="hljs"><span class="hljs-attr">path_to_video_1</span> <span class="hljs-string">label_1</span>
<span class="hljs-attr">path_to_video_2</span> <span class="hljs-string">label_2</span>
<span class="hljs-attr">path_to_video_3</span> <span class="hljs-string">label_3</span>
<span class="hljs-attr">...</span>
<span class="hljs-attr">path_to_video_N</span> <span class="hljs-string">label_N</span>
</code></pre>
<p>All the Kinetics models in the Model Zoo are trained and tested with the same data as <a href="https://github.com/facebookresearch/video-nonlocal-net/blob/master/DATASET.md">Non-local Network</a> and <a href="https://github.com/facebookresearch/SlowFast/blob/master/slowfast/datasets/DATASET.md">PySlowFast</a>. For dataset specific issues, please reach out to the <a href="https://deepmind.com/research/open-source/kinetics">dataset provider</a>.</p>
<h3><a class="anchor" aria-hidden="true" id="charades"></a><a href="#charades" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Charades</h3>
<p>We follow <a href="https://github.com/facebookresearch/SlowFast/blob/master/slowfast/datasets/DATASET.md">PySlowFast</a> to prepare the Charades dataset as follow:</p>
<ol>
<li><p>Download the Charades RGB frames from <a href="http://ai2-website.s3.amazonaws.com/data/Charades_v1_rgb.tar">official website</a>.</p></li>
<li><p>Download the <em>frame list</em> from the following links: (<a href="https://dl.fbaipublicfiles.com/pyslowfast/dataset/charades/frame_lists/train.csv">train</a>, <a href="https://dl.fbaipublicfiles.com/pyslowfast/dataset/charades/frame_lists/val.csv">val</a>).</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="something-something-v2"></a><a href="#something-something-v2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Something-Something V2</h3>
<p>We follow <a href="https://github.com/facebookresearch/SlowFast/blob/master/slowfast/datasets/DATASET.md">PySlowFast</a> to prepare the Something-Something V2 dataset as follow:</p>
<ol>
<li><p>Download the dataset and annotations from <a href="https://20bn.com/datasets/something-something">official website</a>.</p></li>
<li><p>Download the <em>frame list</em> from the following links: (<a href="https://dl.fbaipublicfiles.com/pyslowfast/dataset/ssv2/frame_lists/train.csv">train</a>, <a href="https://dl.fbaipublicfiles.com/pyslowfast/dataset/ssv2/frame_lists/val.csv">val</a>).</p></li>
<li><p>Extract the frames from downloaded videos at 30 FPS. We used ffmpeg-4.1.3 with command:</p>
<pre><code class="hljs"><span class="hljs-attribute">ffmpeg</span> -i <span class="hljs-string">"<span class="hljs-variable">${video}</span>"</span> -r <span class="hljs-number">30</span> -q:v <span class="hljs-number">1</span> <span class="hljs-string">"<span class="hljs-variable">${out_name}</span>"</span>
</code></pre></li>
<li><p>The extracted frames should be organized to be consistent with the paths in frame lists.</p></li>
</ol>
</span></div></article></div><div class="docs-prevnext"></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#data-preparation">Data Preparation</a><ul class="toc-headings"><li><a href="#kinetics">Kinetics</a></li><li><a href="#charades">Charades</a></li><li><a href="#something-something-v2">Something-Something V2</a></li></ul></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div class="footerSection"><div class="social"><a class="github-button" href="https://github.com/facebookresearch/pytorchvideo" data-count-href="https://github.com/facebookresearch/pytorchvideo/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star PytorchVideo on GitHub">pytorchvideo</a></div></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2021 Facebook, Inc<br/>Legal:<a href="https://opensource.facebook.com/legal/privacy/" target="_blank" rel="noreferrer noopener">Privacy</a><a href="https://opensource.facebook.com/legal/terms/" target="_blank" rel="noreferrer noopener">Terms</a></section></footer></div></body></html>