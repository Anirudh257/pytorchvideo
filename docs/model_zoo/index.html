<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>model_zoo · PyTorchVideo</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="## Model Zoo and Benchmarks"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="model_zoo · PyTorchVideo"/><meta property="og:type" content="website"/><meta property="og:url" content="https://pytorchvideo.org/"/><meta property="og:description" content="## Model Zoo and Benchmarks"/><meta property="og:image" content="https://pytorchvideo.org/img/logo.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://pytorchvideo.org/img/logo.svg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo.svg" alt="PyTorchVideo"/><h2 class="headerTitleWithLogo">PyTorchVideo</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/tutorial_overview" target="_self">Tutorials</a></li><li class=""><a href="/docs/index.html" target="_self">Docs</a></li><li class=""><a href="https://github.com/facebookresearch/pytorchvideo/" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">model_zoo</h1></header><article><div><span><h2><a class="anchor" aria-hidden="true" id="model-zoo-and-benchmarks"></a><a href="#model-zoo-and-benchmarks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model Zoo and Benchmarks</h2>
<p>PyTorchVideo provides reference implementation of a large number of video understanding approaches. In this document, we also provide comprehensive benchmarks to evaluate the supported models on different datasets using standard evaluation setup. All the models can be downloaded from the provided links.</p>
<h3><a class="anchor" aria-hidden="true" id="kinetics-400"></a><a href="#kinetics-400" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Kinetics-400</h3>
<table>
<thead>
<tr><th>arch</th><th>depth</th><th>pretrain</th><th>frame length x sample rate</th><th>top 1</th><th>top 5</th><th>Flops (G)</th><th>Params (M)</th><th>Model</th></tr>
</thead>
<tbody>
<tr><td>C2D</td><td>R50</td><td>-</td><td>8x8</td><td>71.46</td><td>89.68</td><td>25.89</td><td>24.33</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/C2D_8x8_R50.pyth">link</a></td></tr>
<tr><td>I3D</td><td>R50</td><td>-</td><td>8x8</td><td>73.27</td><td>90.70</td><td>37.53</td><td>28.04</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/I3D_8x8_R50.pyth">link</a></td></tr>
<tr><td>Slow</td><td>R50</td><td>-</td><td>4x16</td><td>72.40</td><td>90.18</td><td>27.55</td><td>32.45</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOW_4x16_R50.pyth">link</a></td></tr>
<tr><td>Slow</td><td>R50</td><td>-</td><td>8x8</td><td>74.58</td><td>91.63</td><td>54.52</td><td>32.45</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOW_8x8_R50.pyth">link</a></td></tr>
<tr><td>SlowFast</td><td>R50</td><td>-</td><td>4x16</td><td>75.34</td><td>91.89</td><td>36.69</td><td>34.48</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_4x16_R50.pyth">link</a></td></tr>
<tr><td>SlowFast</td><td>R50</td><td>-</td><td>8x8</td><td>76.94</td><td>92.69</td><td>65.71</td><td>34.57</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_8x8_R50.pyth">link</a></td></tr>
<tr><td>SlowFast</td><td>R101</td><td>-</td><td>8x8</td><td>77.90</td><td>93.27</td><td>127.20</td><td>62.83</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_8x8_R101.pyth">link</a></td></tr>
<tr><td>CSN</td><td>R101</td><td>-</td><td>32x2</td><td>77.00</td><td>92.90</td><td>75.62</td><td>22.21</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/CSN_32x2_R101.pyth">link</a></td></tr>
<tr><td>R(2+1)D</td><td>R50</td><td>-</td><td>16x4</td><td>76.01</td><td>92.23</td><td>76.45</td><td>28.11</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/R2PLUS1D_16x4_R50.pyth">link</a></td></tr>
<tr><td>X3D</td><td>XS</td><td>-</td><td>4x12</td><td>69.12</td><td>88.63</td><td>0.91</td><td>3.79</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/X3D_XS.pyth">link</a></td></tr>
<tr><td>X3D</td><td>S</td><td>-</td><td>13x6</td><td>73.33</td><td>91.27</td><td>2.96</td><td>3.79</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/X3D_S.pyth">link</a></td></tr>
<tr><td>X3D</td><td>M</td><td>-</td><td>16x5</td><td>75.94</td><td>92.72</td><td>6.72</td><td>3.79</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/X3D_M.pyth">link</a></td></tr>
</tbody>
</table>
<h3><a class="anchor" aria-hidden="true" id="something-something-v2"></a><a href="#something-something-v2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Something-Something V2</h3>
<table>
<thead>
<tr><th>arch</th><th>depth</th><th>pretrain</th><th>frame length x sample rate</th><th>top 1</th><th>top 5</th><th>Flops (G)</th><th>Params (M)</th><th>Model</th></tr>
</thead>
<tbody>
<tr><td>Slow</td><td>R50</td><td>Kinetics 400</td><td>8x8</td><td>60.04</td><td>85.19</td><td>55.10</td><td>31.96</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/ssv2/SLOW_8x8_R50.pyth">link</a></td></tr>
<tr><td>SlowFast</td><td>R50</td><td>Kinetics 400</td><td>8x8</td><td>61.68</td><td>86.92</td><td>66.60</td><td>34.04</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/ssv2/SLOWFAST_8x8_R50.pyth">link</a></td></tr>
</tbody>
</table>
<h3><a class="anchor" aria-hidden="true" id="charades"></a><a href="#charades" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Charades</h3>
<table>
<thead>
<tr><th>arch</th><th>depth</th><th>pretrain</th><th>frame length x sample rate</th><th>MAP</th><th>Flops (G)</th><th>Params (M)</th><th>Model</th></tr>
</thead>
<tbody>
<tr><td>Slow</td><td>R50</td><td>Kinetics 400</td><td>8x8</td><td>34.72</td><td>55.10</td><td>31.96</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/charades/SLOW_8x8_R50.pyth">link</a></td></tr>
<tr><td>SlowFast</td><td>R50</td><td>Kinetics 400</td><td>8x8</td><td>37.24</td><td>66.60</td><td>34.00</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/charades/SLOWFAST_8x8_R50.pyth">link</a></td></tr>
</tbody>
</table>
<h3><a class="anchor" aria-hidden="true" id="using-pytorchvideo-model-zoo"></a><a href="#using-pytorchvideo-model-zoo" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using PyTorchVideo model zoo</h3>
<p>We provide several different ways to use PyTorchVideo model zoo.</p>
<ul>
<li>The models have been integrated into TorchHub, so could be loaded with TorchHub with or without pre-trained models. Additionally, we provide a <a href="https://pytorchvideo.org/docs/tutorial_torchhub_inference">tutorial</a> which goes over the steps needed to load models from TorchHub and perform inference.</li>
<li>PyTorchVideo models/datasets are also supported in PySlowFast. You can use <a href="https://github.com/facebookresearch/SlowFast/tree/master/projects/pytorchvideo">PySlowFast workflow</a> to train or test PyTorchVideo models/datasets.</li>
<li>You can also use <a href="https://github.com/PyTorchLightning/pytorch-lightning">PyTorch Lightning</a> to build training/test pipeline for PyTorchVideo models and datasets. Please check this <a href="https://pytorchvideo.org/docs/tutorial_classification">tutorial</a> for more information.</li>
</ul>
<p>Notes:</p>
<ul>
<li>The above benchmarks are conducted by <a href="https://github.com/facebookresearch/SlowFast/tree/master/projects/pytorchvideo">PySlowFast workflow</a> using PyTorchVideo datasets and models.</li>
<li>For more details on the data preparation, you can refer to <a href="/docs/data_preparation">PyTorchVideo Data Preparation</a>.</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="pytorchvideo-accelerator-model-zoo"></a><a href="#pytorchvideo-accelerator-model-zoo" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>PytorchVideo Accelerator Model Zoo</h3>
<p>Accelerator model zoo provides a set of efficient models on target device with pretrained checkpoints. To learn more about how to build model, load checkpoint and deploy, please refer to <a href="https://pytorchvideo.org/docs/tutorial_accelerator_use_accelerator_model_zoo">Use PyTorchVideo/Accelerator Model Zoo</a>.</p>
<p><strong>Efficient Models for mobile CPU</strong>
All top1/top5 accuracies are measured with 10-clip evaluation. Latency is benchmarked on Samsung S8 phone with 1s input clip length.</p>
<table>
<thead>
<tr><th>model</th><th>model builder</th><th>top 1</th><th>top 5</th><th>latency (ms)</th><th>params (M)</th><th>checkpoint</th></tr>
</thead>
<tbody>
<tr><td>X3D_XS</td><td>models. accelerator. mobile_cpu. efficient_x3d. EfficientX3d (expansion=&quot;XS&quot;)</td><td>68.5</td><td>88.0</td><td>233</td><td>3.8</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/efficient_x3d_xs_original_form.pyth">link</a></td></tr>
<tr><td>X3D_S</td><td>models. accelerator. mobile_cpu. efficient_x3d. EfficientX3d (expansion=&quot;S&quot;)</td><td>73.0</td><td>90.6</td><td>764</td><td>3.8</td><td><a href="https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/efficient_x3d_s_original_form.pyth">link</a></td></tr>
</tbody>
</table>
</span></div></article></div><div class="docs-prevnext"></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#model-zoo-and-benchmarks">Model Zoo and Benchmarks</a><ul class="toc-headings"><li><a href="#kinetics-400">Kinetics-400</a></li><li><a href="#something-something-v2">Something-Something V2</a></li><li><a href="#charades">Charades</a></li><li><a href="#using-pytorchvideo-model-zoo">Using PyTorchVideo model zoo</a></li><li><a href="#pytorchvideo-accelerator-model-zoo">PytorchVideo Accelerator Model Zoo</a></li></ul></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div class="footerSection"><div class="social"><a class="github-button" href="https://github.com/facebookresearch/pytorchvideo" data-count-href="https://github.com/facebookresearch/pytorchvideo/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star PytorchVideo on GitHub">pytorchvideo</a></div></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2021 Facebook, Inc<br/>Legal:<a href="https://opensource.facebook.com/legal/privacy/" target="_blank" rel="noreferrer noopener">Privacy</a><a href="https://opensource.facebook.com/legal/terms/" target="_blank" rel="noreferrer noopener">Terms</a></section></footer></div></body></html>