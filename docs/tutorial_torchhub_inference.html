<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Running a pre-trained PyTorchVideo classification model using Torch Hub · PyTorchVideo</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="# Introduction"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Running a pre-trained PyTorchVideo classification model using Torch Hub · PyTorchVideo"/><meta property="og:type" content="website"/><meta property="og:url" content="https://pytorchvideo.org/"/><meta property="og:description" content="# Introduction"/><meta property="og:image" content="https://pytorchvideo.org/img/logo.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://pytorchvideo.org/img/logo.svg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo.svg" alt="PyTorchVideo"/><h2 class="headerTitleWithLogo">PyTorchVideo</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/docs/tutorial_overview" target="_self">Tutorials</a></li><li class=""><a href="/docs_redirect" target="_self">Docs</a></li><li class=""><a href="https://github.com/facebookresearch/pytorchvideo/" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Classification</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Tutorials</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/tutorial_overview">Overview</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Classification</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/tutorial_classification">Training a PyTorchVideo classification model</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/docs/tutorial_torchhub_inference">Running a pre-trained PyTorchVideo classification model using Torch Hub</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Accelerator</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/tutorial_accelerator_build_your_model">Build your efficient model with PytorchVideo/Accelerator</a></li><li class="navListItem"><a class="navItem" href="/docs/tutorial_accelerator_use_accelerator_model_zoo">Use PytorchVideo/Accelerator Model Zoo</a></li><li class="navListItem"><a class="navItem" href="/docs/tutorial_accelerator_use_model_transmuter">Accelerate your model with model transmuter in PytorchVideo/Accelerator</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">Running a pre-trained PyTorchVideo classification model using Torch Hub</h1></header><article><div><span><h1><a class="anchor" aria-hidden="true" id="introduction"></a><a href="#introduction" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h1>
<p>PyTorchVideo provides several pretrained models through <a href="https://pytorch.org/hub/">Torch Hub</a>. In this tutorial we will show how to load a pre trained video classification model in PyTorchVideo and run it on a test video. The PyTorchVideo Torch Hub models were trained on the Kinetics 400 [1] dataset.  Available models are described in <a href="https://pytorchvideo.org/docs/model_zoo.html">model zoo documentation</a>.</p>
<p>[1] W. Kay, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.</p>
<p>NOTE: Currently, this tutorial will only work with a local clone of the PyTorchVideo GitHub repo.</p>
<h1><a class="anchor" aria-hidden="true" id="imports"></a><a href="#imports" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Imports</h1>
<pre><code class="hljs css language-python"><span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> Compose, Lambda
<span class="hljs-keyword">from</span> torchvision.transforms._transforms_video <span class="hljs-keyword">import</span> (
    CenterCropVideo,
    NormalizeVideo,
)
<span class="hljs-keyword">from</span> pytorchvideo.data.encoded_video <span class="hljs-keyword">import</span> EncodedVideo
<span class="hljs-keyword">from</span> pytorchvideo.transforms <span class="hljs-keyword">import</span> (
    ApplyTransformToKey,
    ShortSideScale,
    UniformTemporalSubsample
)
</code></pre>
<h1><a class="anchor" aria-hidden="true" id="load-model"></a><a href="#load-model" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Load Model</h1>
<p>Let's select the <code>slow_r50</code> model which was trained using a 8x8 setting on the Kinetics 400 dataset.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># Device on which to run the model</span>
device = <span class="hljs-string">"cuda:0"</span>

<span class="hljs-comment"># Pick a pretrained model</span>
model_name = <span class="hljs-string">"slow_r50"</span>

<span class="hljs-comment"># Local path to the parent folder of hubconf.py in the pytorchvideo codebase</span>
path = <span class="hljs-string">'../'</span>
model = torch.hub.load(path, source=<span class="hljs-string">"local"</span>, model=model_name, pretrained=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Set to eval mode and move to desired device</span>
model = model.eval()
model = model.to(device)
</code></pre>
<h1><a class="anchor" aria-hidden="true" id="setup-labels"></a><a href="#setup-labels" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setup Labels</h1>
<p>Next let's download the id-to-label mapping for the Kinetics 400 dataset on which the torch hub models were trained. This will be used to get the category label names from the predicted class ids.</p>
<pre><code class="hljs css language-python">!wget https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json
</code></pre>
<pre><code class="hljs css language-python"><span class="hljs-keyword">with</span> open(<span class="hljs-string">"kinetics_classnames.json"</span>, <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> f:
    kinetics_classnames = json.load(f)

<span class="hljs-comment"># Create an id to label name mapping</span>
kinetics_id_to_classname = {}
<span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> kinetics_classnames.items():
    kinetics_id_to_classname[v] = str(k).replace(<span class="hljs-string">'"'</span>, <span class="hljs-string">""</span>)
</code></pre>
<h1><a class="anchor" aria-hidden="true" id="input-transform"></a><a href="#input-transform" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Input Transform</h1>
<p>Before passing the video into the model we need to apply some input transforms and sample a clip of the correct duration. We will define them below.</p>
<pre><code class="hljs css language-python">side_size = <span class="hljs-number">256</span>
mean = [<span class="hljs-number">0.45</span>, <span class="hljs-number">0.45</span>, <span class="hljs-number">0.45</span>]
std = [<span class="hljs-number">0.225</span>, <span class="hljs-number">0.225</span>, <span class="hljs-number">0.225</span>]
crop_size = <span class="hljs-number">256</span>
num_frames = <span class="hljs-number">8</span>
sampling_rate = <span class="hljs-number">8</span>
frames_per_second = <span class="hljs-number">30</span>

<span class="hljs-comment"># Note that this transform is specific to the slow_R50 model.</span>
<span class="hljs-comment"># If you want to try another of the torch hub models you will need to modify this transform</span>
transform =  ApplyTransformToKey(
    key=<span class="hljs-string">"video"</span>,
    transform=Compose(
        [
            UniformTemporalSubsample(num_frames),
            Lambda(<span class="hljs-keyword">lambda</span> x: x/<span class="hljs-number">255.0</span>),
            NormalizeVideo(mean, std),
            ShortSideScale(
                size=side_size
            ),
            CenterCropVideo(crop_size=(crop_size, crop_size))
        ]
    ),
)

<span class="hljs-comment"># The duration of the input clip is also specific to the model.</span>
clip_duration = (num_frames * sampling_rate)/frames_per_second
</code></pre>
<h1><a class="anchor" aria-hidden="true" id="load-an-example-video"></a><a href="#load-an-example-video" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Load an example video</h1>
<p>We can now test the model with an example video from the Kinetics validation set such as this <a href="https://www.youtube.com/watch?v=3and4vWkW4s">archery video</a>.</p>
<p>We will load the video and apply the input transform.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># Download the example video file</span>
!wget https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4
</code></pre>
<pre><code class="hljs css language-python"><span class="hljs-comment"># Load the example video</span>
video_path = <span class="hljs-string">"archery.mp4"</span>

<span class="hljs-comment"># Select the duration of the clip to load by specifying the start and end duration</span>
<span class="hljs-comment"># The start_sec should correspond to where the action occurs in the video</span>
start_sec = <span class="hljs-number">0</span>
end_sec = start_sec + clip_duration

<span class="hljs-comment"># Initialize an EncodedVideo helper class</span>
video = EncodedVideo.from_path(video_path)

<span class="hljs-comment"># Load the desired clip</span>
video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)

<span class="hljs-comment"># Apply a transform to normalize the video input</span>
video_data = transform(video_data)

<span class="hljs-comment"># Move the inputs to the desired device</span>
inputs = video_data[<span class="hljs-string">"video"</span>]
inputs = inputs.to(device)
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="get-model-predictions"></a><a href="#get-model-predictions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Get model predictions</h3>
<p>Now we are ready to pass the input into the model and classify the action.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># Pass the input clip through the model</span>
preds = model(inputs[<span class="hljs-literal">None</span>, ...])
</code></pre>
<p>Let's look at the top 5 best predictions:</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># Get the predicted classes</span>
post_act = torch.nn.Softmax(dim=<span class="hljs-number">1</span>)
preds = post_act(preds)
pred_classes = preds.topk(k=<span class="hljs-number">5</span>).indices

<span class="hljs-comment"># Map the predicted classes to the label names</span>
pred_class_names = [kinetics_id_to_classname[int(i)] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> pred_classes[<span class="hljs-number">0</span>]]
print(<span class="hljs-string">"Predicted labels: %s"</span> % <span class="hljs-string">", "</span>.join(pred_class_names))
</code></pre>
<h1><a class="anchor" aria-hidden="true" id="conclusion"></a><a href="#conclusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h1>
<p>In this tutorial we showed how to load and run a pretrained PyTorchVideo model on a test video. You can run this tutorial as a notebook in the PyTorchVideo tutorials directory.</p>
<p>To learn more about PyTorchVideo, check out the rest of the <a href="https://pytorchvideo.org/docs/">documentation</a>  and <a href="https://pytorchvideo.org/docs/tutorial_overview">tutorials</a>.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/docs/tutorial_classification"><span class="arrow-prev">← </span><span class="function-name-prevnext">Training a PyTorchVideo classification model</span></a><a class="docs-next button" href="/docs/tutorial_accelerator_build_your_model"><span class="function-name-prevnext">Build your efficient model with PytorchVideo/Accelerator</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div class="footerSection"><div class="social"><a class="github-button" href="https://github.com/facebookresearch/pytorchvideo" data-count-href="https://github.com/facebookresearch/pytorchvideo/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star PytorchVideo on GitHub">pytorchvideo</a></div></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2021 Facebook, Inc<br/>Legal:<a href="https://opensource.facebook.com/legal/privacy/" target="_blank" rel="noreferrer noopener">Privacy</a><a href="https://opensource.facebook.com/legal/terms/" target="_blank" rel="noreferrer noopener">Terms</a></section></footer></div></body></html>